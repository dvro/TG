% !TEX encoding = ISO-8859-1
\chapter{Técnicas de Seleção de Protótipos}
% \label{ch:tecnicasdeselecaodeprototipos}

Neste capítulo, serão mostradas as técnicas de seleção de protótipos abordadas neste trabalho. Cada uma das sessões abaixo abordará uma técnica, será mostrado o conceito da técnica, assim como o pseudo-código e as caracterísicas de cada uma destas técnicas.

\section{ENN}

Edited Nearest Neighbor Rule\cite{enn:2011} é uma técnica de seleção de protótipos puramente seletiva proposta por Wilson em 1976. De uma forma geral, esta técnica foi projetada para funcionar como um filtro de ruídos, ela elimina pontos na região de fronteira, região de alta susceptibilidade a erros, e com isso elimina ruídos.\

Por atuar apenas na região de fronteira, esta técnica possui uma baixa capacidade de redução, deixando as instâncias que não se encontram na região de fronteira intactas, exceto pelos ruídos extremos.\

Uma desvantagem desta técnica é que ela possui uma baixa capacidade de redução de elementos, visto que ela não elimina redundância.

Segue abaixo o algoritmo da execução do ENN e, logo após, alguns comentários sobre este algoritmo.

\begin{algorithm}
\caption{ENN}
\label{pseudocode_enn}
\begin{algorithmic}[1]
\REQUIRE {$list$: uma lista}
\FORALL {instância $e_i$ da base de dados original}
\STATE Aplique o KNN sobre $e_i$
\IF {$e_i$ foi classificado erroneamente}
\STATE salve $e_i$ em $list$
\ENDIF
\ENDFOR
\STATE Remova da base de dados todos os elementos que estão em $list$
\end{algorithmic}
\end{algorithm}

O valor de K pode variar de acordo com o tamanho da base de dados, porém, tipicamente, utiliza-se o valor de K=3. Tipicamente, O valor de K é inversamente proporcional a quantidade de instâncias que serão eliminadas, ou seja, para que o filtro elimine todos os possíveis ruídos, deve-se utilizar K=1.

%% As figuras, obtidas de \cite{com.cs.mcgill.cs.godfried}, exemplificarão o funcionamento do ENN.

%\begin{figure}
%\includegraphics[scale=0.55]{imagens/enn1.png}
%\caption{Base de dados com região de indecisão, K=1}
%\label{fig:enn1}
%\end{figure}

%\begin{figure}
%\includegraphics[scale=0.55]{imagens/enn2.png}
%\caption{Base de dados com região de indecisão, K=3}
%\label{fig:enn2}
%\end{figure}

%% TODO COLOCAR ALGUMA FIGURA PARA EXEMPLIFICAR
%% TODO EXPLICAR A FIGURA

Uma vantagem do ENN é que ele independe da ordem que a base de dados foi apresentada, ou seja, o ENN aplicado a uma base de dados, com o mesmo valor de K, sempre terá o mesmo resultado.

%% TODO CITAR DESVANTAGENS DO ENN 

\section{Tomek Links}
\section{CNN}

Condensed Nearest Neighbor \cite{cnn:1968} é uma técnica de seleção de protótipos puramente seletiva que tem como objetivo eliminar informação redundante. Diferentemente do ENN \cite{enn:2011}, o CNN não elimina instâncias nas regiões de fronteira, a técnica mantém estes elementos pois estes que "são importantes" para distinguir entre duas classes.

A ideia geral do CNN é encontrar o menor subconjunto da base de dados original que, utilizando o 1-NN, classifica todos os padrões da base de dados original corretamente. Fazendo isso, o algoritmo elimina os elementos mais afastados da região de indecisão, da fronteira de classificação.

O algoritmo do CNN será mostrado abaixo, e logo após, comentários a respeito do mesmo.

\begin{algorithm}
\caption{CNN}
\label{pseudocode_cnn}
\begin{algorithmic}[1]
\REQUIRE {$list$: uma lista}
\STATE Escolha um elemento de cada classe $aleatoreamente$ e coloque-os em $list$
\FORALL {instância $e_i$ da base de dados original}
\STATE Aplique o KNN sobre $e_i$ utilizando os elementos em $list$ para treinamento
\IF {$e_i$ foi classificado erroneamente}
\STATE salve $e_i$ em $list$
\ENDIF
\ENDFOR
\STATE Remova da base original todos os elementos que não estão em $list$
\end{algorithmic}
\end{algorithm}

Podemos observar que este algoritmo possui uma abordagem diferente do ENN, por exemplo, pois ele começa com um conjunto mínimo de instâncias (uma de cada classe) e depois adiciona instâncias conforme a necessidade de mantê-las para que todos os elementos da base de dados original sejam classificados corretamente.

Uma coisa que pode-se observar no algorítmo, é a palavra $aleatoriamente$, o que significa que o CNN aplicado numa mesma base de dados com um mesmo valor de K para o KNN, nem sempre resulta nos mesmos protótipos. O primeiro fato para que isso ocorra é a seleção aleatória dos protótipos iniciais. Existem algumas adaptações para o CNN, onde os protótipos iniciais são escolhidos utilizando técnicas como o SGP\cite{fayed:sgp} para obter as instâncias mais centrais. Modificações no CNN são muito comuns \cite{cnn:1976}, porém, mesmo com estas modificações, o CNN ainda não é determinístico, pois a ordem em que as instâncias são classificadas afeta o resultado final.

%% TODO COLOCAR ALGUMA FIGURA PARA EXEMPLIFICAR
%% TODO EXPLICAR A FIGURA

Para o caso de estudo abordado neste trabalho, o CNN pode ser utilizado de forma adaptada. A adaptação consiste em manter todos os elementos da classe minoritária e o mínimo possível da classe marjoritária. O próprio CNN se encarrega de remover os elementos redundantes da classe marjoritária, assim, basta apenas selecionar todos os elementos da classe minoritária aos protótipos iniciais.
Segue abaixo o algoritmo desta adaptação:

\begin{algorithm}
\caption{CNN para bases desbalanceadas}
\label{pseudocode_cnn}
\begin{algorithmic}[1]
\REQUIRE {$list$: uma lista}
\STATE Coloque todos os elementos da classe minoritária em $list$
\FORALL {instância $e_i$ da base de dados original}
\STATE Aplique o KNN sobre $e_i$ utilizando os elementos em $list$ para treinamento
\IF {$e_i$ foi classificado erroneamente}
\STATE salve $e_i$ em $list$
\ENDIF
\ENDFOR
\STATE Remova da base original todos os elementos que não estão em $list$
\end{algorithmic}
\end{algorithm}

Com o algorítmo citado acima, os elementos redundantes da classe marjoritária são removidos, e todos os elementos da classe minoritária são mantidos. Esta adaptação do CNN irá reduzir a base de dados, ocasionando as vantagens de redução, e ainda reduzirá o desbalenceamento da base.

\section{LVQ}
\subsection{LVQ 1}
\subsection{LVQ 2.1}
\subsection{LVQ 3}
\section{SGP}
\section{SGP 2}
\section{CCNN}


