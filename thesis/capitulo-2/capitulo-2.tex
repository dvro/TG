% !TEX encoding = ISO-8859-1
\chapter{Técnicas de Seleção de Protótipos}
% \label{ch:tecnicasdeselecaodeprototipos}

Neste capítulo, serão mostradas as técnicas de seleção de protótipos abordadas neste trabalho. Cada uma das sessões abaixo abordará uma técnica, será mostrado o conceito da técnica, assim como o pseudo-código e as caracterísicas de cada uma destas técnicas.

\section{ENN}

Edited Nearest Neighbor Rule\cite{enn:2011} é uma técnica de seleção de protótipos puramente seletiva proposta por Wilson em 1976. De uma forma geral, esta técnica foi projetada para funcionar como um filtro de ruídos, ela elimina pontos na região de fronteira, região de alta susceptibilidade a erros, e com isso elimina ruídos.\

Por atuar apenas na região de fronteira, esta técnica possui uma baixa capacidade de redução, deixando as instâncias que não se encontram na região de fronteira intactas, exceto pelos ruídos extremos.\

Uma desvantagem desta técnica é que ela possui uma baixa capacidade de redução de elementos, visto que ela não elimina redundância.

Segue abaixo o algoritmo da execução do ENN e, logo após, alguns comentários sobre este algoritmo.

\begin{algorithm}
\caption{ENN}
\label{pseudocode_enn}
\begin{algorithmic}[1]
\REQUIRE {$list$: uma lista}
\FORALL {instância $e_i$ da base de dados original}
\STATE Aplique o KNN sobre $e_i$
\IF {$e_i$ foi classificado erroneamente}
\STATE salve $e_i$ em $list$
\ENDIF
\ENDFOR
\STATE Remova da base de dados todos os elementos que estão em $list$
\end{algorithmic}
\end{algorithm}

O valor de K pode variar de acordo com o tamanho da base de dados, porém, tipicamente, utiliza-se o valor de K=3. Tipicamente, O valor de K é inversamente proporcional a quantidade de instâncias que serão eliminadas, ou seja, para que o filtro elimine todos os possíveis ruídos, deve-se utilizar K=1.

%% TODO COLOCAR ALGUMA FIGURA PARA EXEMPLIFICAR
%% TODO EXPLICAR A FIGURA

Uma vantagem do ENN é que ele independe da ordem que a base de dados foi apresentada, ou seja, o ENN aplicado a uma base de dados, com o mesmo valor de K, sempre terá o mesmo resultado.

%% TODO CITAR DESVANTAGENS DO ENN 

\section{CNN}

Condensed Nearest Neighbor \cite{cnn:1968} é uma técnica de seleção de protótipos puramente seletiva que tem como objetivo eliminar informação redundante. Diferentemente do ENN \cite{enn:2011}, o CNN não elimina instâncias nas regiões de fronteira, a técnica mantém estes elementos pois estes que "são importantes" para distinguir entre duas classes.

A ideia geral do CNN é encontrar o menor subconjunto da base de dados original que, utilizando o 1-NN, classifica todos os padrões da base de dados original corretamente. Fazendo isso, o algoritmo elimina os elementos mais afastados da região de indecisão, da fronteira de classificação.

O algoritmo do CNN será mostrado abaixo, e logo após, comentários a respeito do mesmo.

\begin{algorithm}
\caption{CNN}
\label{pseudocode_cnn}
\begin{algorithmic}[1]
\REQUIRE {$list$: uma lista}
\STATE Escolha um elemento de cada classe $aleatoreamente$ e coloque-os em $list$
\FORALL {instância $e_i$ da base de dados original}
\STATE Aplique o KNN sobre $e_i$ utilizando os elementos em $list$ para treinamento
\IF {$e_i$ foi classificado erroneamente}
\STATE salve $e_i$ em $list$
\ENDIF
\ENDFOR
\STATE Remova da base original todos os elementos que não estão em $list$
\end{algorithmic}
\end{algorithm}

Podemos observar que este algoritmo possui uma abordagem diferente do ENN, por exemplo, pois ele começa com um conjunto mínimo de instâncias (uma de cada classe) e depois adiciona instâncias conforme a necessidade de mantê-las para que todos os elementos da base de dados original sejam classificados corretamente.

Uma coisa que pode-se observar no algoritmo, é a palavra $aleatoriamente$, o que significa que o CNN aplicado numa mesma base de dados com um mesmo valor de K para o KNN, nem sempre resulta nos mesmos protótipos. O primeiro fato para que isso ocorra é a seleção aleatória dos protótipos iniciais. Existem algumas adaptações para o CNN, onde os protótipos iniciais são escolhidos utilizando técnicas como o SGP\cite{fayed:sgp} para obter as instâncias mais centrais. Modificações no CNN são muito comuns \cite{cnn:1976}, porém, mesmo com estas modificações, o CNN ainda não é determinístico, pois a ordem em que as instâncias são classificadas afeta o resultado final.

%% TODO COLOCAR ALGUMA FIGURA PARA EXEMPLIFICAR
%% TODO EXPLICAR A FIGURA

Para o caso de estudo abordado neste trabalho, o CNN pode ser utilizado de forma adaptada. A adaptação consiste em manter todos os elementos da classe minoritária e o mínimo possível da classe marjoritária. O próprio CNN se encarrega de remover os elementos redundantes da classe marjoritária, assim, basta apenas selecionar todos os elementos da classe minoritária aos protótipos iniciais.
Segue abaixo o algoritmo desta adaptação:

\begin{algorithm}
\caption{CNN para bases desbalanceadas}
\label{pseudocode_cnn}
\begin{algorithmic}[1]
\REQUIRE {$list$: uma lista}
\STATE Coloque todos os elementos da classe minoritária em $list$
\FORALL {instância $e_i$ da base de dados original}
\STATE Aplique o KNN sobre $e_i$ utilizando os elementos em $list$ para treinamento
\IF {$e_i$ foi classificado erroneamente}
\STATE salve $e_i$ em $list$
\ENDIF
\ENDFOR
\STATE Remova da base original todos os elementos que não estão em $list$
\end{algorithmic}
\end{algorithm}

Com o algoritmo CNN adaptado para bases desbalanceadas, os elementos redundantes da classe marjoritária são removidos, e todos os elementos da classe minoritária são mantidos. Esta adaptação do CNN irá reduzir a base de dados, ocasionando as vantagens de redução, e ainda reduzirá o desbalenceamento da base.


\section{Tomek Links}

Mantendo a mesma linha do ENN, Tomek Links é uma técnica de seleção de protótipos puramente seletiva que elimina os elementos das regiões de fronteiras e instâncias com probabilidade de ser ruído. Tomek Links podem ser definidos da seguinte forma: Dadas duas instâncias $e_i$ e $e_j$, o par \textit{($e_i$, $e_j$)} é chamado de Tomek Link se não existe nenhuma instância $e_k$, tal que, para todo $e_k$ \textit{dist($e_i$,$e_j$) < dist($e_i$,$e_k$)} e \textit{dist($e_i$,$e_j$) < dist($e_j$,$e_k$)}. Segue abaixo o algorítmo detalhado:

\begin{algorithm}
\caption{Seleciona Tomek Links}
\label{pseudocode_cnn}
\begin{algorithmic}[1]
\REQUIRE {$list$: uma lista}
	\FORALL {instância $e_i$ da base de dados original}
		\STATE $e_j$ = instância mais próxima de $e_i$
		\IF {instância mais p¿oxima de $e_j$ for $e_i$}
			\IF {classe de $e_i$ for diferente da classe de $e_j$}
				\STATE salve o par \textit{($e_i$, $e_j$)} em $list$
			\ENDIF
		\ENDIF
	\ENDFOR
	\STATE Retorne os pares em $list$, os Tomek Links
\end{algorithmic}
\end{algorithm}



Os Tomek Links representam elementos da região de fronteira e prováveis ruídos, e a técnica de seleção de protótipos consiste em remover os Tomek Links da base de dados original. Segue abaixo o algoritmo da seleção de protótipos:



\begin{algorithm}
\caption{Tomek Links}
\label{pseudocode_cnn}
\begin{algorithmic}[1]
\REQUIRE {$list$: uma lista}
\STATE $list$ = $Seleciona Tomek Links$ da base original
	\FORALL {\textit{($e_i$, $e_j$)} em $list$}
		\STATE remova $e_i$ da base original
		\STATE remova $e_j$ da base original
	\ENDFOR
	\STATE Retorne a base original filtrada
\end{algorithmic}
\end{algorithm}



Enquanto o CNN remove os elementos que estão longe da região de indecisão, o Tomek Links remove os elementos que estão próximos desta região, o que causa uma maior separação entre as classes.



%% TODO FIGURA PARA TOMEK LINKS
%% EXPLICAR FIGURA



Observa-se facilmente que os Tomek Links pode remove todas as instâncias da fronteira, inclusive as instâncias da classe minoritária, assim sendo, uma possível adaptação dos Tomek Links é eliminar apenas os elementos das classes marjoritárias. Nesse caso, ainda ocorreria uma separação entre as classes, mas apenas as instâncias da classe marjoritária seriam removidos, dimunuindo assim o nível de desbalanceamento. Segue abaixo o algoritmo desta adaptação:



\begin{algorithm}
\caption{Tomek Links}
\label{pseudocode_cnn}
\begin{algorithmic}[1]
\REQUIRE {$list$: uma lista}
	\STATE $list$ = $Seleciona Tomek Links$ da base original
	\FORALL {\textit{($e_i$, $e_j$)} em $list$}
		\IF {$e_i$ for da classe marjoritária}
			\STATE remova $e_i$ da base original
		\ENDIF
		\IF {$e_j$ for da classe marjoritária}
			\STATE remova $e_j$ da base original
		\ENDIF
	\ENDFOR
	\STATE Retorne a base original filtrada
\end{algorithmic}
\end{algorithm}



Com esta adaptação, a classe minoritária é mantida, evitando o aumento do desbalanceamento ou a remoção por alta probabilidade de ruído.



\section{LVQ}
\subsection{LVQ 1}
\subsection{LVQ 2.1}
\subsection{LVQ 3}
\section{SGP}
\section{SGP 2}
\section{CCNN}


