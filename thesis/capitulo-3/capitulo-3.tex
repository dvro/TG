% !TEX encoding = ISO-8859-1
\chapter{Análise em Bases Artificiais}
\label{ch:analiseembasesartificiais}

Neste capítulo será analisado o desempenho de cada técnica de seleção de protótipos em bases desbalanceadas artificiais. Além da taxa de acerto, será considerada a quantidade de protótipos gerados por cada técnica e a disposição destes protótipos.

\section{Bases Artificiais}
\label{sec:basesartificiais}

Para avaliar de forma visual o comportamento de cada técnica, foram selecionadas bases artificiais com diferentes níveis de sobrebosição entre a classe marjoritária e a classe minotirária. Também foram selecionados dois diferentes níveis de desbalanceamento, um onde a classe minoritária possui aproximadamente 10\% das instâncias e outro onde possui apenas 5\%.

No apêndice \ref{ch:appendixa}, página \pageref{ch:appendixa}, estão as figuras das bases de dados geradas artificialmente que serão utilizadas neste trabalho. Na figura \ref{fig:orig10} estão os diferentes níveis de sobreposição para uma base onde a classe minoritária possui 10\% das instâncias. O mesmo acontece na figura \ref{fig:orig5}, onde a classe minoritária possui apenas 5\% das instâncias.

Foi utilizado dois níveis de desbalanceamento diferentes para avaliar se as técnicas tem o mesmo comportamento quando o nível de desbalanceamento é alterado.


\section{Análise Individual}

Nesta sessão, cada técnica sera analisada isoladamente. Para exemplificar e demonstrar as falhas de cada técnica, serão utilizadas figuras e tabelas que comprovem o comportamento de cada algoritmo diante de bases desbalanceadas.

No experimentos desta sessão, foi utilizado o próprio conjunto de treinamento como conjunto de testes, afim de avaliar a representatividade do conjunto de treinamento original. O teste com \textit{K-fold Cross-Validation} será feito em bases reais no próximo capítulo.

\subsection{ENN}
\label{subsec:ennembasesartificiais}

	Conforme dito no capítulo anterior, o ENN é uma técnica que elimina ruídos na fronteira de classificação, mantendo as instâncias que apresentam redundância de informação. Na tabela \ref{tab:enn10}, podemos ver a taxa de acerto do ENN na base em que classe minoritária possui 10\% das instâncias, e onde o nível de sobreposição varia gradualmente, sendo o último nível o caso onde a classe minoritária está totalmente imersa na classe marjoritária.

	Observa-se que o ENN, aparentemente, manteve uma boa taxa de acerto, sendo o pior caso com 90.00\%. Porém, percebe-se que esta taxa diminui conforme o nível de sobreposição aumenta, na mesma proporção em que instâncias são eliminadas, e isso acontece por conta da eliminação da classe minoritária.

	Ao observar a taxa de acerto da classe minoritária, pode-se perceber que o ENN eliminou muito mais instâncias da classe minoritária que da marjoritária. Confrome o nível de sobreposição aumentou, mais instâncias da classe minoritária foram removidas, chegando ao caso onde a taxa de acerto desta classe foi de 0.00\%.

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Nível de Sobreposição    &   I   &   II  &   III &   IV  &   V   \\
\hline %----- linha horizontal
Acerto total            &   100.00\%   &   100.00\%   &   95.80\%    &   90.00\%   &   90.00\%   \\
\hline
Acerto marjoritária     &   100.00\%   &   100.00\%   &   97.89\%    &   95.56\%   &   100.00\%   \\
\hline
Acerto minoritária      &   100.00\%   &   100.00\%   &   77.00\%    &   40.00\%   &   0.00\%   \\
\hline
Tamanho resultante      &   100.00\%   &   100.00\%   &   92.00\%    &   82.00\%   &   81.00\%   \\
\hline
\end{tabular}%--- fechamento do ambiente tabular
\end{center}   %fim da centralização da tabela
\caption{ENN com nível de desbalanceamento 10\%}
\label{tab:enn10}
\end{table}

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Nível de Sobreposição    &   I   &   II  &   III &   IV  &   V   \\
\hline %----- linha horizontal
Acerto total            &   100.00\%   &   100.00\%   &   97.68\%    &   94.74\%   &   94.74\%   \\
\hline
Acerto marjoritária     &   100.00\%   &   100.00\%   &   99.00\%    &   98.33\%   &   100.00\%   \\
\hline
Acerto minoritária      &   100.00\%   &   100.00\%   &   74.00\%    &   30.00\%   &   0.00\%   \\
\hline
Tamanho resultante      &   100.00\%   &   100.00\%   &   95.79\%    &   90.53\%   &   90.00\%   \\
\hline
\end{tabular}%--- fechamento do ambiente tabular
\end{center}   %fim da centralização da tabela
\caption{ENN com nível de desbalanceamento 5\%}
\label{tab:enn5}
\end{table}

A tabela \ref{tab:enn5} mostra que, com uma base mais desbalanceadas, o decaimento da taxa de acerto da classe minoritária foi maior. Enquanto quando a classe minoritária possuia 10\% das instâncias a taxa de acerto desta classe no nível de sobreposição IV foi de 40\%, quando esta classe possuia 5\% das instâncias, a taxa de acerto dela, neste mesmo nível, foi de apenas 30\%.

\begin{figure}[H]
\center
\includegraphics[scale=0.40]{imagens/outputs/ENN_10_15.eps}
\caption{ENN sobre sobre base com total sobreposição de classes e 10\% de desbalanceamento}
\label{fig:enn1015}
\end{figure}

A figura \ref{fig:enn1015} mostra o que aconteceu após a aplicação do ENN. Todas as instâncias da classe minoritária foram removidas, pois foram tratadas como ruídos. Quanto maior o nível de desbalanceamento, mais inviável a técnica se torna, pois, conforme a sobreposição aumenta, mais instâncias são removidas, e a classe minoritária tende a desaparecer mais rapidamente que a marjoritária.

Quando não existe sobreposição de classes, o ENN pode ser uma boa opção para remoção de ruídos, porém, quando existe sobreposição, esta técnica pode remover praticamente todas as instâncias da classe minoritária, aumentando o nível de desbalanceamento e deixando o classificador inviável para identificação da classe de interesse.


Uma possível adaptação do ENN seria remover apenas instâncias da classe marjoritária, com isso, o nível de desbalanceamento seria diminuido e a taxa de acerto da classe minoritária não seria afetada em relação ao conjunto original.

\subsection{CNN}

O CNN, diferentemente do ENN, possui a abordagem de remover instâncias redundantes, e não ruídos. O comportamento esperado é que o CNN remova muitas instâncias da classe marjoritária e poucas instâncias da classe minoritária. Dependendo do nível de sobreposição das classes, porém, o CNN pode gerar diferentes resultados. Para os experimentos abaixo foi utilizado o valor de K = 3, pois com K = 1 a taxa de acerto seria de 100\%.

Observando a tabela \ref{tab:cnn10} percebe-se que o CNN obteve uma boa taxa de acerto total, mesmo no caso de bases desbalanceadas. O CNN também teve um alto nível de redução de instâncias. Quando não houve sobreposição, o CNN reduziu a base para 0.50\% do tamanho original, e para 13.90\% quando houve nível máximo de sobreposição.

O resultado da tabela não é uniforme pelo fato de o CNN não ser determinístico, ou seja, depende da ordem em que os dados são apresentados. Porém, pode-se observar que, no geral, o CNN obteve uma boa taxa de acerto para a classe minoritária.



\begin{table}[H]
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Nível de Sobreposição    &   I   &   II  &   III &   IV  &   V   \\
\hline %----- linha horizontal
Acerto total            &   90.00\%   &   99.30\%   &   94.70\%    &   91.00\%   &   90.40\%   \\
\hline
Acerto marjoritária     &   100.00\%   &   99.44\%   &   96.22\%    &   94.44\%   &   95.00\%   \\
\hline
Acerto minoritária      &   0.00\%   &   98.00\%   &   81.00\%    &   60.00\%   &   49.00\%   \\
\hline
Tamanho resultante      &   0.50\%   &   1.20\%   &   7.50\%    &   13.30\%   &   13.90\%   \\
\hline
\end{tabular}%--- fechamento do ambiente tabular
\end{center}   %fim da centralização da tabela
\caption{CNN com nível de desbalanceamento 10\%}
\label{tab:cnn10}
\end{table}

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Nível de Sobreposição    &   I   &   II  &   III &   IV  &   V   \\
\hline %----- linha horizontal
Acerto total            &   94.74\%   &   99.26\%   &   97.47\%    &   94.74\%   &   94.11\%   \\
\hline
Acerto marjoritária     &   100.00\%   &   100.00\%   &   98.67\%    &   97.56\%   &   95.89\%   \\
\hline
Acerto minoritária      &   0.00\%   &   86.00\%   &   76.00\%    &   44.00\%   &   62.00\%   \\
\hline
Tamanho resultante      &   0.32\%   &   0.63\%   &   4.32\%    &   7.68\%   &   7.47\%   \\
\hline
\end{tabular}%--- fechamento do ambiente tabular
\end{center}   %fim da centralização da tabela
\caption{CNN com nível de desbalanceamento 5\%}
\label{tab:cnn5}
\end{table}

Aumentando o nível de desbalanceamento (ver tabela \ref{tab:cnn5}), houve uma melhora na taxa de acerto da classe minoritária em todos os níveis de sobreposição. Isso se deve ao maior agrupamento das instâncias desta classe.


Pode-se observar que o CNN reduz muito a base de dados quando não há sobreposição entre as classes, pois garante que as instâncias selecionadas são suficiente para classificar corretamente todo o conjunto de treinamento quando K = 1. Porém, neste experimento, a taxa de acerto da classe minoritária quando não houve sobreposição foi mínima. Isso aconteceu porque foi utilizado K = 3, e CNN deixou apenas 1 instância da classe minoritária, assim, todas as instâncias desta classe foram classificadas erroneamente. O caso pode ser visto na figura \ref{fig:cnn50}.

\begin{figure}[H]
\center
\includegraphics[scale=0.40]{imagens/outputs/CNN_5_0.eps}
\caption{CNN sobre sobre base sem sobreposição de classes e 5\% de desbalanceamento}
\label{fig:cnn50}
\end{figure}


\begin{figure}[H]
\center
\includegraphics[scale=0.40]{imagens/outputs/CNN_5_5.eps}
\caption{CNN sobreposição III de classes e 5\% de desbalanceamento}
\label{fig:cnn55}
\end{figure}

Observando a figura \ref{fig:cnn55}, também é possível ver que o CNN remove instâncias redundantes, então, a região de sobreposição é praticamente mantida. Isso mostra que, o CNN colabora para diminuir o nível de desbalanceamento, principalmente diante de classes sobrepostas. Todavia, em casos onde é importante identificar a classe minoritária, é necessário utilizar uma técnica de pré-processamento para melhor delimitar esta classe na região.

Outra opção é utilizar a versão do CNN utilizada pelo \textit{One-Sided Selection}, assim, as instâncias da classe minoritária são priorizadas.

\subsection{Tomek Links}

O Tomek Links é uma técnica de seleção de protótipos determinística que atua na região de fronteira. Esta técnica é muito utilizada como pré-processamento de dados, visto que ela remove instâncias duvidosas, mas, normalmente, não tem alto nível de redução.

Observando a tabela \ref{tab:tl10}, percebe-se que o Tomek Links não reduziu a base de dados quando não há sobreposição de classes, assim, a taxa de acerto é a máxima. Porém, conforme a região de sobreposição aumenta, mais instâncias são removidas, e com isso, a taxa de acerto começa a ser afetada.

Esta técnica possui uma boa taxa de acerto média, mas a taxa de acerto da classe minoritária diminui conforme o nível de sobreposição aumenta. Isso acontece porque com a eliminação de um Tomek Link uma instância de cada classe é eliminada, e a classe minoritária diminui ainda mais, dificultando a classificação correta desta classe.

Com um nível de desbalanceamento menor, mais instâncias da classe minoritária são mantidas em regiões onde não há sobreposição.


\begin{table}[H]
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Nível de Sobreposição    &   I   &   II  &   III &   IV  &   V   \\
\hline %----- linha horizontal
Acerto total            &   100.00\%   &   100.00\%   &   97.10\%    &   95.30\%   &   94.00\%   \\
\hline
Acerto marjoritária     &   100.00\%   &   100.00\%   &   98.33\%    &   97.11\%   &   97.33\%   \\
\hline
Acerto minoritária      &   100.00\%   &   100.00\%   &   86.00\%    &   79.00\%   &   64.00\%   \\
\hline
Tamanho resultante      &   100.00\%   &   100.00\%   &   94.20\%    &   90.40\%   &   89.00\%   \\
\hline
\end{tabular}%--- fechamento do ambiente tabular
\end{center}   %fim da centralização da tabela
\caption{Tomek Links com nível de desbalanceamento 10\%}
\label{tab:tl10}
\end{table}

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Nível de Sobreposição    &   I   &   II  &   III &   IV  &   V   \\
\hline %----- linha horizontal
Acerto total            &   100.00\%   &   100.00\%   &   98.21\%    &   96.21\%   &   97.26\%   \\
\hline
Acerto marjoritária     &   100.00\%   &   100.00\%   &   99.22\%    &   98.00\%   &   99.00\%   \\
\hline
Acerto minoritária      &   100.00\%   &   100.00\%   &   80.00\%    &   64.00\%   &   66.00\%   \\
\hline
Tamanho resultante      &   100.00\%   &   100.00\%   &   97.68\%    &   94.53\%   &   94.32\%   \\
\hline
\end{tabular}%--- fechamento do ambiente tabular
\end{center}   %fim da centralização da tabela
\caption{Tomek Links com nível de desbalanceamento 5\%}
\label{tab:tl5}
\end{table}

A tabela \ref{tab:tl5} mostra que, com o mesmo nível de sobreposição, quando maior o nível de desbalanceamento (quanto menos instâncias da classe minoritária) menos Tomek Links são feitos, e menos instâncias são eliminadas. Assim, existe uma menor redução de instâncias.

Com relação a taxa de acerto da classe minoritária, em média, quanto menos instâncias desta classe, menor a taxa de acerto dela. Isso ocorre porque quanto menor a percentagem de instâncias da classe minoritária, maior é a importancia de uma instância para a representação da classe.


\begin{figure}[H]
\center
\includegraphics[scale=0.40]{imagens/outputs/TomekLinks_10_5.eps}
\caption{Tomek Links sobreposição III de classes e 10\% de desbalanceamento}
\label{fig:tomeklinks105}
\end{figure}

Na figura \ref{fig:tomeklinks105}, está mostrado o resultado do \textit{Tomek Links} aplicado numa base de dados com 10\% da classe minoritária e 90\% da classe marjoritária. O nível de sobreposição é o nível III, ou seja, a classe minoritária está parcialmente imersa na classe marjoritária. Observando a figura, percebe-se que o \textit{Tomek Links} remove instâncias da região de sobreposição, e mantém instâncias redundantes.

Com isso, conclui-se que o Tomek Links pode ser utilizado como uma técnica de pré-processamento, pois ainda existe muitos dados redundantes, principalmente da classe marjoritária. 

Uma opção é utilizar o Tomek Links alternativo apresentado em Algorithm \ref{alg:tomek_links_for_unbalanced_datasets}, página \pageref{alg:tomek_links_for_unbalanced_datasets}, porque com isso o nível de desbalanceamento seria reduzido. Outra vantagem é que a região de sobreposição seria classificada como da classe minoritária, caso de interesse na maioria das bases de dados.


\subsection{OSS}

Conforme dito anteriormente, o \textit{One-Sided Selection} é uma técnica muito utilizada como pré-processamento e é apropriada para bases desbalanceadas. Inicialmente, com a aplicação do CNN adaptado para bases desbalanceadas, instâncias redundantes da classe marjoritária são removidas. Depois, com a aplicação do Tomek Links, também adaptado para bases desbalanceadas, as instâncias da classe marjoritária que estão na fronteira são removidas.

O OSS mantém as instâncias da classe minoritária, com isso ele favorece a identificação desta classe. Observando a tabela \ref{tab:oss10}, percebe-se que a taxa de acerto total é alta, tendo 87.50\% no pior caso, e isso independe do nível de intersecção. Para a classe minoritária, vê-se que a taxa de acerto foi alta, especialmente quando existe um baixo nível de sobreposição de classes.

O poder de redução do OSS também é alto, indo para 17.40\% da base original no pior caso de redução. Neste mesmo caso, o nível de desbalanceamento é reduzido, chegando a ter praticamente a mesma quantidade de instâncias em cada classe.


\begin{table}[H]
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Nível de Sobreposição    &   I   &   II  &   III &   IV  &   V   \\
\hline %----- linha horizontal
Acerto total            &   97.50\%   &   98.80\%   &   95.20\%    &   85.90\%   &   87.50\%   \\
\hline
Acerto marjoritária     &   97.22\%   &   98.67\%   &   95.22\%    &   85.78\%   &   88.00\%   \\
\hline
Acerto minoritária      &   100.00\%   &   100.00\%   &   95.00\%    &   87.00\%   &   83.00\%   \\
\hline
Tamanho resultante      &   10.40\%   &   11.10\%   &   13.60\%    &   15.50\%   &   17.40\%   \\
\hline
\end{tabular}%--- fechamento do ambiente tabular
\end{center}   %fim da centralização da tabela
\caption{OSS com nível de desbalanceamento 10\%}
\label{tab:oss10}
\end{table}

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Nível de Sobreposição    &   I   &   II  &   III &   IV  &   V   \\
\hline %----- linha horizontal
Acerto total            &   98.74\%   &   99.58\%   &   97.05\%    &   93.37\%   &   93.89\%   \\
\hline
Acerto marjoritária     &   98.67\%   &   99.56\%   &   97.11\%    &   93.44\%   &   94.22\%   \\
\hline
Acerto minoritária      &   100.00\%   &   100.00\%   &   96.00\%    &   92.00\%   &   88.00\%   \\
\hline
Tamanho resultante      &   5.58\%   &   5.89\%   &   7.16\%    &   8.63\%   &   9.68\%   \\
\hline
\end{tabular}%--- fechamento do ambiente tabular
\end{center}   %fim da centralização da tabela
\caption{OSS com nível de desbalanceamento 5\%}
\label{tab:oss5}
\end{table}

Observando a tabela \ref{tab:oss5}, percebe-se que houve um maior nível de redução, e a taxa de acerto da classe minoritária foi ainda maior, mostrando que, para bases altamente desbalanceadas, o OSS é uma técnica ainda mais eficiente. 

\begin{figure}[H]
\center
\includegraphics[scale=0.40]{imagens/outputs/OSS_5_5.eps}
\caption{OSS com sobreposição III de classes e 5\% de desbalanceamento}
\label{fig:oss55}
\end{figure}

A figura \ref{fig:oss55} exemplifica a aplicação do \textit{One-Sided Selection} sobre uma base de dados onde a classe minoritária possui apenas 5\% das instâncias e metade das instâncias desta classe estão na região de indecisão. As instâncias da classe marjoritária redudantes foram eliminadas, o que aumenta a capacidade de generalização dos protótipos. Outro detalhe perceptível é que a região de indecisão é tomada pela classe minoritária.

Por estas caracterísitcas, o OSS é uma técnica muito apropriada para bases desbalanceadas, além de ser eficiente no que se refere a redução de instâncias. Porém, uma desvantagem é que a classe minoritária fica com instâncias redudantes após a aplicação, por isso, o OSS é muito utilizado como técnica de pré-processamento de dados para outras técnicas de seleção de protótipos.

\subsection{LVQ}

O \textit{Learning Vector Quantization} é uma técnica criativa de seleção de protótipos. Diferentemente das técnicas examinadas nas subsessões anteriores, esta técnica cria novas instâncias a partir do conjunto de treinamento.

Para os experimentos das versões do LVQ, foram utilizados 1000 iterações, $\alpha(t)= 0.02 \times e^{(-t/200)}$, sendo $t$ o número da iteração, $w = 0.70$ e $\epsilon = 0.30$. Os protótipos iniciais utilizados foram 5 protótipos para cada classe, sendo eles próximos da mediana da classe correspondente.


\subsubsection{LVQ 1}

O LVQ 1 obteve uma boa taxa de acerto geral, sendo sempre acima de 70\%. Observa-se também que o taxa de acerto tende a decrescer conforme o nível de sobreposição de classes aumenta. Observando a tabela \ref{tab:lvq110}, percebe-se que a taxa de acerto total decresce conforme o nível de sobreposição aumenta. Isso acontece porque a taxa de acerto da classe marjoritária decresce, enquanto a taxa de acerto da classe minoritária se mantém alta (neste experimento 100\%).

A taxa de acerto da classe minoritária é alta porque as instâncias desta classe estão muito próximas, então, durante o ajuste de protótipos, os mesmos ficam bem ajustados, em posições estratégicas. 

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Nível de Sobreposição    &   I   &   II  &   III &   IV  &   V   \\
\hline %----- linha horizontal
Acerto total            &   95.30\%   &   87.40\%   &   80.80\%    &   77.00\%   &   71.90\%   \\
\hline
Acerto marjoritária     &   94.78\%   &   86.00\%   &   78.67\%    &   74.44\%   &   68.78\%   \\
\hline
Acerto minoritária      &   100.00\%   &   100.00\%   &   100.00\%    &   100.00\%   &   100.00\%   \\
\hline
Tamanho resultante      &   1.00\%   &   1.00\%   &   1.00\%    &   1.00\%   &   1.00\%   \\
\hline
\end{tabular}%--- fechamento do ambiente tabular
\end{center}   %fim da centralização da tabela
\caption{LVQ 1 com nível de desbalanceamento 10\%}
\label{tab:lvq110}
\end{table}

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Nível de Sobreposição    &   I   &   II  &   III &   IV  &   V   \\
\hline %----- linha horizontal
Acerto total            &   93.47\%   &   91.79\%   &   84.84\%    &   74.74\%   &   77.68\%   \\
\hline
Acerto marjoritária     &   93.11\%   &   91.33\%   &   84.00\%    &   73.33\%   &   76.44\%   \\
\hline
Acerto minoritária      &   100.00\%   &   100.00\%   &   100.00\%    &   100.00\%   &   100.00\%   \\
\hline
Tamanho resultante      &   1.05\%   &   1.05\%   &   1.05\%    &   1.05\%   &   1.05\%   \\
\hline
\end{tabular}%--- fechamento do ambiente tabular
\end{center}   %fim da centralização da tabela
\caption{LVQ1 com nível de desbalanceamento 5\%}
\label{tab:lvq15}
\end{table}

A tabela \ref{tab:lvq15} confirma que, mesmo com um maior nível de desbalanceamento, a taxa de acerto teve uma melhora em relação a menos desbalanceada. Isso aconteceu porque a região da classe minoritária está mais compacta, assim, o espalhamento da classe marjoritária foi maior, garantindo uma melhor taxa de acerto para esta.

\begin{figure}[H]
\center
\includegraphics[scale=0.40]{imagens/outputs/LVQ1_5_5.eps}
\caption{LVQ 1 com sobreposição de classes III e 5\% de desbalanceamento}
\label{fig:lvq155}
\end{figure}

A figura \ref{fig:lvq155} mostra que, em baixo nível de sobreposição, os protótipos ficam em torno do centróide, e o seu espalhamento depende totalmente do nível de espalhamento das classes. Já a figura \ref{fig:lvq1515} mostra que o mesmo acontece quando há total sobreposição, porém, o LVQ 1 tende a dividir as classes. Neste caso, os protótipos da classe marjoritária ficaram mais espalhados, porém, afastados da região de indecisão, por isso, a taxa de acerto da classe minoritária é alta.

\begin{figure}[H]
\center
\includegraphics[scale=0.40]{imagens/outputs/LVQ1_5_15.eps}
\caption{LVQ 1 com sobreposição de classes V e 5\% de desbalanceamento}
\label{fig:lvq1515}
\end{figure}

Comparando a figura \ref{fig:lvq11015} com a figura \ref{fig:lvq1515}, percebe-se que, quanto maior a quantidade de instâncias da classe minoritária, mais espalhados ficam os protótipos, e melhor delimitada fica a região desta classe.

\begin{figure}[H]
\center
\includegraphics[scale=0.40]{imagens/outputs/LVQ1_10_15.eps}
\caption{LVQ 1 com sobreposição de classes V e 10\% de desbalanceamento}
\label{fig:lvq11015}
\end{figure}

O LVQ1 se mostra eficiente para bases desbalanceadas, principalmente no que se refere a classificar bem instâncias da classe minoritária. Uma grande vantagem é que a quantidade de protótipos resultante pode ser pré-definida, com isso, o desbalanceamento pode ser reduzido ou até mesmo eliminado. É recomendado também que a quantidade de protótipos seja proporcional ao nível de espalhamento de cada classe, quanto mais espalhada, mais protótipos são necessários para definir a classe.

\subsubsection{LVQ 2.1}

O LVQ 2.1 também teve uma boa taxa de acerto média, e assim como o LVQ 1, teve taxa de acerto máxima para classe minoritária em todos os níveis de sobreposição de classes. Esses dados podem ser observados na tabela \ref{tab:lvq25} e \ref{tab:lvq210}.


\begin{table}[H]
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Nível de Sobreposição    &   I   &   II  &   III &   IV  &   V   \\
\hline %----- linha horizontal
Acerto total            &   95.80\%   &   90.40\%   &   81.60\%    &   80.70\%   &   76.20\%   \\
\hline
Acerto marjoritária     &   95.33\%   &   89.33\%   &   79.56\%    &   78.56\%   &   73.56\%   \\
\hline
Acerto minoritária      &   100.00\%   &   100.00\%   &   100.00\%    &   100.00\%   &   100.00\%   \\
\hline
Tamanho resultante      &   1.00\%   &   1.00\%   &   1.00\%    &   1.00\%   &   1.00\%   \\
\hline
\end{tabular}%--- fechamento do ambiente tabular
\end{center}   %fim da centralização da tabela
\caption{LVQ 2.1 com nível de desbalanceamento 10\%}
\label{tab:lvq210}
\end{table}


\begin{table}[H]
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Nível de Sobreposição    &   I   &   II  &   III &   IV  &   V   \\
\hline %----- linha horizontal
Acerto total            &   90.84\%   &   89.68\%   &   83.89\%    &   80.11\%   &   76.84\%   \\
\hline
Acerto marjoritária     &   90.33\%   &   89.11\%   &   83.00\%    &   79.00\%   &   75.56\%   \\
\hline
Acerto minoritária      &   100.00\%   &   100.00\%   &   100.00\%    &   100.00\%   &   100.00\%   \\
\hline
Tamanho resultante      &   1.05\%   &   1.05\%   &   1.05\%    &   1.05\%   &   1.05\%   \\
\hline
\end{tabular}%--- fechamento do ambiente tabular
\end{center}   %fim da centralização da tabela
\caption{LVQ 2.1 com nível de desbalanceamento 5\%}
\label{tab:lvq25}
\end{table}

Conforme citado anteriormente, o LVQ 2.1 evita uma divergência entre os protótipos, isto pode ser observado em leves diferenças entre a figura \ref{fig:lvq2100} e a figura \ref{fig:lvq1100}. 

\begin{figure}[H]
\center
	\mbox{%
		\subfigure[LVQ 1]{\label{fig:lvq1100}%
			\includegraphics[scale=0.30]{imagens/outputs/LVQ1_10_0.eps}}
		\subfigure[LVQ 2.1]{\label{fig:lvq2100}%
			\includegraphics[scale=0.30]{imagens/outputs/LVQ2.1_10_0.eps}}
	}
  \caption{LVQ 1 e 2.1 com 10\% da classe minoritária e nível I de sobreposição}
  \label{fig:lvqcomp}
\end{figure}


Observa-se que os protótipos do LVQ 2.1 estão levemente menos espalhados que o LVQ 1, isso afeta a taxa de acerto da classe marjoritária (comparar tabela \ref{tab:lvq110} e \ref{tab:lvq210}). Assim, é interessante utilizar o LVQ 2.1 quando se deseja um menor espalhamento das instâncias, normalmente, quando existe uma distribuição uniforme das classes.

\subsubsection{LVQ 3}

O LVQ 3 tem o objetivo de evitar o sobreajuste do LVQ 2.1. Observando as tabelas \ref{tab:lvq310} e \ref{tab:lvq35}, percebe-se que o LVQ 3 manteve em média o mesmo desempenho que o LVQ 2.1, porém, a diferença de desempenho está em bases de baixo nível de sobreposição entre as classes.

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Nível de Sobreposição    &   I   &   II  &   III &   IV  &   V   \\
\hline %----- linha horizontal
Acerto total            &   94.70\%   &   88.10\%   &   89.40\%    &   75.60\%   &   73.50\%   \\
\hline
Acerto marjoritária     &   94.11\%   &   86.78\%   &   88.22\%    &   72.89\%   &   70.56\%   \\
\hline
Acerto minoritária      &   100.00\%   &   100.00\%   &   100.00\%    &   100.00\%   &   100.00\%   \\
\hline
Tamanho resultante      &   1.00\%   &   1.00\%   &   1.00\%    &   1.00\%   &   1.00\%   \\
\hline
\end{tabular}%--- fechamento do ambiente tabular
\end{center}   %fim da centralização da tabela
\caption{LVQ 3 com nível de desbalanceamento 10\%}
\label{tab:lvq310}
\end{table}

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Nível de Sobreposição    &   I   &   II  &   III &   IV  &   V   \\
\hline %----- linha horizontal
Acerto total            &   95.79\%   &   92.32\%   &   83.68\%    &   78.32\%   &   74.53\%   \\
\hline
Acerto marjoritária     &   95.56\%   &   91.89\%   &   82.78\%    &   77.11\%   &   73.11\%   \\
\hline
Acerto minoritária      &   100.00\%   &   100.00\%   &   100.00\%    &   100.00\%   &   100.00\%   \\
\hline
Tamanho resultante      &   1.05\%   &   1.05\%   &   1.05\%    &   1.05\%   &   1.05\%   \\
\hline
\end{tabular}%--- fechamento do ambiente tabular
\end{center}   %fim da centralização da tabela
\caption{LVQ 3 com nível de desbalanceamento 5\%}
\label{tab:lvq35}
\end{table}

Comparando a figura \ref{fig:lvq3100} com a figura \ref{fig:lvq2100} e a \ref{fig:lvq1100}, observa-se que o LVQ 3 é o meio termo entre o LVQ 1 e o LVQ 2.1. Assim, recomenda-se o uso do LVQ 3 em bases desbalanceadas em casos onde o nível de sobreposição de classes é baixo e deseja-se conter o nível de espalhamento entre as classes.

\begin{figure}[H]
\center
\includegraphics[scale=0.40]{imagens/outputs/LVQ3_10_0.eps}
\caption{LVQ 3 sem sobreposição de classes e 10\% de desbalanceamento}
\label{fig:lvq3100}
\end{figure}

A escolha da versão do \textit{Learning Vector Quantization} é tão importante quanto a escolha dos parâmetros de forma apropriada. Nestes experimentos foram utilizados os mesmos valores para os parâmetros em comum, então, as conclusões se restrigem ao uso de cada técnica em bases desbalanceadas, mas, experimentalmente, verifica-se que a escolha adequada de parâmetros é tão importante quanto a escolha da versão do LVQ.

\subsection{SGP 1}

A primeira versão do \textit{Self-Generating Prototypes} é uma técnica de alto poder de redução muito completa para bases com distribuições multi-modais de classes. Porém, após os experimentos realizados, foi possível identificar uma série de falhas nesta técnica.

Conforme citado no capítulo anterior, o SGP possui um fator de generalização. Neste experimento, foi utilizado $R_n = 0.05$ e $R_s = 0.05$, a escolha de valores foi feita de acordo com o proposto em trabalhos relacionados\cite{csp:sgp}. Para classificar, foi utilizado o KNN com K=1, pois a ideia é que haja uma única instância que represente um grande conjunto de instâncias.

O poder de redução desta técnica é muito alto, observa-se nas tabelas \ref{tab:sgp110} e \ref{tab:sgp15} que, mesmo em alto nível de sobreposição, a quantidade de protótipos não pasou de 5.9\% da base original.

O SGP, sem fator de generalização, apresenta uma taxa de 100\% de acerto sobre o conjunto de treinamento\cite{fayed:sgp}. Porém, ao introduzir o fator de generalização o SGP perde esta propriedade. Observando a tabela \ref{tab:sgp110}, percebe-se que, mesmo com fator de generalização, o SGP 1 obteve boas taxas de acerto para baixo nível de sobreposição. Porém, para um alto nível de sobreposição (nível V), a classe minoritária obteve 0.00\% de taxa de acerto, enquanto que a classe marjoritária obteve 100\%. O mesmo acontece na tabela \ref{tab:sgp15}, porém, como o desbalanceamento é mais acentuado, a taxa de acerto da classe minoritária foi 0\% nos níveis IV e V de sobreposição.


\begin{table}[H]
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Nível de Sobreposição    &   I   &   II  &   III &   IV  &   V   \\
\hline %----- linha horizontal
Acerto total            &   100.00\%   &   100.00\%   &   95.40\%    &   90.70\%   &   90.00\%   \\
\hline
Acerto marjoritária     &   100.00\%   &   100.00\%   &   97.89\%    &   95.78\%   &   100.00\%   \\
\hline
Acerto minoritária      &   100.00\%   &   100.00\%   &   73.00\%    &   45.00\%   &   0.00\%   \\
\hline
Tamanho resultante      &   0.40\%   &   3.50\%   &   5.80\%    &   8.20\%   &   5.90\%   \\
\hline
\end{tabular}%--- fechamento do ambiente tabular
\end{center}   %fim da centralização da tabela
\caption{SGP 1 com nível de desbalanceamento 10\%}
\label{tab:sgp110}
\end{table}

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Nível de Sobreposição    &   I   &   II  &   III &   IV  &   V   \\
\hline %----- linha horizontal
Acerto total            &   100.00\%   &   100.00\%   &   97.68\%    &   94.74\%   &   94.74\%   \\
\hline
Acerto marjoritária     &   100.00\%   &   100.00\%   &   98.89\%    &   100.00\%   &   100.00\%   \\
\hline
Acerto minoritária      &   100.00\%   &   100.00\%   &   76.00\%    &   0.00\%   &   0.00\%   \\
\hline
Tamanho resultante      &   0.53\%   &   2.74\%   &   5.26\%    &   5.16\%   &   5.68\%   \\
\hline
\end{tabular}%--- fechamento do ambiente tabular
\end{center}   %fim da centralização da tabela
\caption{SGP 1 com nível de desbalanceamento 5\%}
\label{tab:sgp15}
\end{table}


\begin{figure}[H]
\center
\includegraphics[scale=0.40]{imagens/outputs/SGP_5_15.eps}
\caption{SGP 1 com total sobreposição de classes e 5\% de desbalanceamento}
\label{fig:sgp1515}
\end{figure}

Observando a figura \ref{fig:sgp1515}, conclui-se que o fator de generalização fez com que todas as instâncias da classe minoritária fossem eliminadas. Em baixo nível de sobreposição isso não acontece pois as instâncias da classe minoritária são representadas por um mesmo protótipo, porém, conforme o nível de sobreposição aumenta, mais protótipos são necessários para representar esta classe, assim, menos instâncias por protótipos. Quando o algoritmo principal termina, o fator de generalização elimina os grupos que possuem menos que $R_s \times S$, sendo $S$ a quantidade de instâncias do maior grupo. Assim, as instâncias vão sendo removidas independentemente de classe.

Observa-se também que, quando o nível de desbalanceamento é mais acentuado, este efeito ocorre mais rapidamente, pois são menos instâncias por protótipos gerados. Assim, em bases desbalanceadas onde a classe minoritária representa 1\%, por exemplo, provavelmente todas as instâncias destas classes seriam eliminadas, independente do nível de sobreposição. Caso não seja utilizado o fator de generalização, estas instâncias são mantidas, porém, o SGP 1 manterá todos os ruídos, levando a erros de classificação.

Esta é uma falha muito grave, porque apesar de ter uma alta taxa de acerto geral e capacidade de redução, o SGP possui péssimas taxas de acerto para a classe minoritária, chegando até a errar em todos os casos.

Uma solução para o SGP é que o fator de generalização seja mantido, porém, com uma alteração. o $R_n$ eliminaria grupos com poucas instâncias dependendo, não da quantidade de instâncias do maior grupo, mas sim, da quantidade de instâncias do maior grupo da mesma classe. Com esta alteração, a classe minoritária não seria extinta e a taxa de acerto da mesma seria beneficiada.

\subsection{SGP 2}

A segunda versão do \textit{Self-Generating Prototypes} apresenta os mesmos defeitos da primeira versão. Porém, ela apresenta algumas caracterísitcas interessantes que serão abordadas.

Observando as tabelas \ref{tab:sgp210} e \ref{tab:sgp5}, percebe-se que o SGP 2 obteve uma excelente taxa de acerto geral, porém, a taxa de acerto da classe minoritária é altamente prejudicada no momento da generalização. As possíveis adaptações para este problema já foram citados na sub-sessão anterior.

Quanto a quantidade de protótipos, observa-se que o SGP 2 tem um poder de redução muito maior que o SGP 1, isso acontece devido ao acrescimo do \textit{Pruning} e \textit{Merge}. Em geral o SGP 2 obteve uma taxa de acerto semelhante ao SGP 1, porém, em média, apenas metade dos protótipos foram necessários.

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Nível de Sobreposição    &   I   &   II  &   III &   IV  &   V   \\
\hline %----- linha horizontal
Acerto total            &   100.00\%   &   100.00\%   &   95.50\%    &   90.00\%   &   90.20\%   \\
\hline
Acerto marjoritária     &   100.00\%   &   100.00\%   &   98.22\%    &   100.00\%   &   95.56\%   \\
\hline
Acerto minoritária      &   100.00\%   &   100.00\%   &   71.00\%    &   0.00\%   &   42.00\%   \\
\hline
Tamanho resultante      &   0.30\%   &   1.90\%   &   2.90\%    &   2.70\%   &   4.40\%   \\
\hline
\end{tabular}%--- fechamento do ambiente tabular
\end{center}   %fim da centralização da tabela
\caption{SGP 2 com nível de desbalanceamento 10\%}
\label{tab:sgp210}
\end{table}

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Nível de Sobreposição    &   I   &   II  &   III &   IV  &   V   \\
\hline %----- linha horizontal
Acerto total            &   100.00\%   &   100.00\%   &   97.58\%    &   94.74\%   &   94.74\%   \\
\hline
Acerto marjoritária     &   100.00\%   &   100.00\%   &   98.78\%    &   100.00\%   &   100.00\%   \\
\hline
Acerto minoritária      &   100.00\%   &   100.00\%   &   76.00\%    &   0.00\%   &   0.00\%   \\
\hline
Tamanho resultante      &   0.21\%   &   1.26\%   &   2.42\%    &   2.63\%   &   2.63\%   \\
\hline
\end{tabular}%--- fechamento do ambiente tabular
\end{center}   %fim da centralização da tabela
\caption{SGP 2 com nível de desbalanceamento 5\%}
\label{tab:sgp25}
\end{table}


Comparando-se a figura \ref{fig:sgp2515} com a figura \ref{fig:sgp1515}, observa-se que o SGP 2 manteve a mesma região representada com uma quantidade menor de instâncias. O $Merge$ é um algoritmo determinístico e muito eficiente, já o $Pruning$ não é determinístico, e é necessário um limite de poda para que não aconteça um sobreajuste.

\begin{figure}[H]
\center
\includegraphics[scale=0.40]{imagens/outputs/SGP2_5_15.eps}
\caption{SGP 2 com total sobreposição de classes e 5\% de desbalanceamento}
\label{fig:sgp2515}
\end{figure}

Conclui-se que o SGP 2 é superior ao SGP 1 no que se refere a generalização e redução de protótipos. Porém, esta técnica apresenta os mesmos defeitos do SGP 1, sendo necessária a mesma adaptação citada anteriormente para que a classe minoritária, normalmente o caso de maior interesse, possa ser identificada.

